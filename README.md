## Steps to run the project
1. Install python3 and pip3
2. Install virtualenv
    ```commandline
    pip3 install virtualenv
    ```
3. Decompress the data folder into the root of the project
4. Install the requirements
    ```commandline
    python3 -m venv ~/venv 
    source ~/venv/bin/activate 
    pip install -r requirements.txt
    ```
5. Add project root to PYTHONPATH
    ```commandline
    export PYTHONPATH=$PYTHONPATH:$(pwd)
    ```
6. Run the project
    ```commandline
    python3 src/main.py
    ```

# Questions
### Senior Data Engineer - Technical Test 

#### Question 1
Your company uses API calls to receive clients requests for FX trade orders. You are asked to define a possible solution to be able to feed the streaming logs generated by the API calls received into a ML model that detects anomalies in real time.
Describe the architecture you think would best serve this purpose, using the resources you see fit.
Please provide a PDF file containing:

* The architecture diagram.
* For each software module used in the diagram, a brief description (2 lines max per
module) explaining what the module does as well as its inputs and outputs.
* Briefly describe the reasoning behind your choices.

#### Question 2
The data analyst in your company is preparing a dashboard for the visualisation of client data to be used by the customer success team to review the clients' activity. In order to do so, they need client order amounts normalised to EUR. You are given a sample of historical daily FX rates and a sample of the client orders data.

* The FX rates data is retrieved from a third party service and must be cleaned and prepared for its subsequent use. Write a script using PySpark that performs the following transformations to the raw FX rates data:
  * Read the data from csv 
  * Detect possible outliers (a simple approach will suffice) 
  * Fill missing rates 
  * Ensure that the final rates pool includes all currency pairs in both directions 
  * Save the data in parquet format

   For this item, you should provide:
   1. The PySpark script you wrote.
   2. The parquet file generated by your script.
* In a second PySpark script, use the preprocessed FX rates data to normalise the amounts in the orders csv to EUR, using the rate of the day in which the order was received and write the output in parquet format. The final amount should be expressed using the correct number of decimal points specified in the data provided. Take into account that the dashboard will read this data through queries that use the creation date as their main filter.


 Please provide the scripts for the transformations as well as the transformed data in the correct format.
#### Question 3
Write a Cloudformation/Terraform project to deploy your solution on AWS. You are free to use any tool to know to help you to deploy the solution with the minimum human interaction.
